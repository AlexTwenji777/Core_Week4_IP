# -*- coding: utf-8 -*-
"""Core_Week_4_IP_Alex_Twenji.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vMQzGQ9sIlU7tU3Tsz4CNK9D6wiatcGb

# DESCRIBING THE QUESTION

## 1. SPECIFYING THE QUESTION

Determine if the mean of the number of blue_cars taken from postcodes starting with ‘75’ is at least similar to that of all the Paris postcodes. To investigate this, our hypothesis will be: 

1. The Null Hypothesis is that the mean of blue_cars taken in postcodes starting with ‘75’ is greater than or equal to that of all the Paris postcodes.

2. The Alternate Hypothesis is that the mean of blue_cars taken in postcodes starting with ‘75’ during the is less than that of all the Paris postcodes.



## 2. DEFINING THE METRIC FOR SUCCESS

It will be deemed successful if the Null hypothesis fails to be rejected, i.e. if it is true.


## 3. EXPERIMENTAL DESIGN

a) Loading Datasets and Preparing the Data.

b) Data Cleaning to deal with Anomalies and Outliers.

c) Exploratory Data Analysis (Univariate and Bivariate Analysis).

d) Hypothesis Testing to Implement the Solution.

e) Conclusions and Recommendation.

# DATA PREPARATION
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from scipy import stats
from scipy.stats import norm
# %matplotlib inline

# Description of the features.

Description = pd.read_excel('http://bit.ly/DSCoreAutolibDatasetGlossary')
Description

# Dataset we'll work with.

autolib = pd.read_csv('http://bit.ly/DSCoreAutolibDataset')
autolib.head()

# Understanding the dataset parameters we'll primarily be working with.

autolib['Postal code'].nunique()

autolib.tail()

# Confirming dates are from January to June.

autolib.date.unique()

print('The dataset has ' + str(autolib.shape[0]) + ' rows and ' + str(autolib.shape[1]) + ' columns')

# Checking for null values

autolib.isnull().sum()

autolib.describe()

autolib.describe(include='all')

"""# DATA CLEANING

Checking for Validity, Accuracy, Completeness, Consistency and Uniformity of the Data.
"""

# The columns need to be synchronized e.g remove the space in Postal

autolib.head(1)

autolib.columns = autolib.columns.str.lower().str.replace(' ', '_')
autolib.head(1)

# We can also correct dayofweek to make it more readable

autolib.columns = autolib.columns.str.lower().str.replace('dayofweek', 'day_of_week')
autolib.head(1)

# The types of data are appropriate 

autolib.info()

# There are no null values

autolib.isnull().any()

# There are no duplicates

autolib.duplicated().sum()

"""# DATA ANALYSIS

## 1. EXPLORATORY DATA ANALYSIS (EDA)

### 1.1 UNIVARIATE

**a) Numerical**
"""

autolib.info()

col_names = ['bluecars_taken_sum','bluecars_returned_sum', 'slots_freed_sum', 'slots_taken_sum']

fig, ax = plt.subplots(len(col_names), figsize= (8,40))

for i, col_val in enumerate(col_names):
  sns.boxplot(y = autolib[col_val], ax= ax[i])
  ax[i].set_title('Box plot - {}'.format(col_val), fontsize= 10)
  ax[i].set_xlabel(col_val, fontsize= 8)
plt.show()

# From the boxplots below it can be seen that there are a lot of outliers.

# Checking for Outliers

Q1_bluecars_taken_sum = autolib['bluecars_taken_sum'].quantile(.25)
Q3_bluecars_taken_sum = autolib['bluecars_taken_sum'].quantile(.75)

IQR_bluecars_taken_sum = Q3_bluecars_taken_sum - Q1_bluecars_taken_sum


anomalies = autolib[(autolib.bluecars_taken_sum < Q1_bluecars_taken_sum - 1.5* IQR_bluecars_taken_sum)  |
                    (autolib.bluecars_taken_sum > Q3_bluecars_taken_sum + 1.5 * IQR_bluecars_taken_sum) ]
print('BlueCar taken outliers are: ' + str(anomalies.bluecars_taken_sum.count()))

Q1_bluecars_returned_sum = autolib['bluecars_returned_sum'].quantile(.25)
Q3_bluecars_returned_sum = autolib['bluecars_returned_sum'].quantile(.75)

IQR_bluecars_returned_sum = Q3_bluecars_returned_sum - Q1_bluecars_returned_sum


anomalies = autolib[(autolib.bluecars_returned_sum < Q1_bluecars_returned_sum - 1.5* IQR_bluecars_returned_sum)  |
                    (autolib.bluecars_returned_sum > Q3_bluecars_returned_sum + 1.5 * IQR_bluecars_returned_sum) ]
print('BlueCar returned outliers are: ' + str(anomalies.bluecars_returned_sum.count()))

Q1_slots_freed_sum = autolib['slots_freed_sum'].quantile(.25)
Q3_slots_freed_sum = autolib['slots_freed_sum'].quantile(.75)

IQR_slots_freed_sum = Q3_slots_freed_sum - Q1_slots_freed_sum


anomalies = autolib[(autolib.slots_freed_sum < Q1_slots_freed_sum - 1.5* IQR_slots_freed_sum)  |
                    (autolib.slots_freed_sum > Q3_slots_freed_sum + 1.5 * IQR_slots_freed_sum) ]
print('Slots freed outliers are: ' + str(anomalies.slots_freed_sum.count()))

Q1_slots_taken_sum = autolib['slots_taken_sum'].quantile(.25)
Q3_slots_taken_sum = autolib['slots_taken_sum'].quantile(.75)

IQR_slots_taken_sum = Q3_slots_taken_sum - Q1_slots_taken_sum


anomalies = autolib[(autolib.slots_taken_sum < Q1_slots_taken_sum - 1.5* IQR_slots_taken_sum)  |
                    (autolib.slots_taken_sum > Q3_slots_taken_sum + 1.5 * IQR_slots_taken_sum) ]
print('Slots taken outliers are: ' + str(anomalies.slots_taken_sum.count()))

"""From the above, the outliers are too many to remove as this will affect the accuracy of the data analysis, and the result could be inconclusive and/or incorrect.

**b) Categorical**
"""

autolib.info()

# Records of the week (weekday and weekend)

autolib.day_type.value_counts().plot.bar()

"""The weekday contains the majority of the data, suggesting most of the activity occurs then. Due to this, we'll be using the weekday for our analysis. during the hypothesis testing. Furthermore, since we'll be using the z-score, the larger the data, the more accurate the results will be.

**c) Summary Statistics**
"""

autolib.describe()

# Central Tendancies 

# mean
print('The mean of Bluecars taken: ' +str(autolib.bluecars_taken_sum.mean()))
print('The mean of Bluecars returned: ' +str(autolib.bluecars_returned_sum.mean()))
print('The mean of slots freed: ' +str(autolib.slots_freed_sum.mean()))
print('The mean of slots taken: ' +str(autolib.slots_taken_sum.mean()))

# The means of bluecars taken and those returned are almost similar.
# The means of slots taken and those freed are also almost similar.

# median
print('The median of Bluecars taken: ' +str(autolib.bluecars_taken_sum.median()))
print('The median of Bluecars returned: ' +str(autolib.bluecars_returned_sum.median()))
print('The median of slots freed: ' +str(autolib.slots_freed_sum.median()))
print('The median of slots taken: ' +str(autolib.slots_taken_sum.median()))

# The medians of bluecars taken and those returned are similar.
# The means of slots taken and those freed are also similar.

# mode
print('The mode of Bluecars taken: ' +str(autolib.bluecars_taken_sum.mode()))
print('The mode of Bluecars returned: ' +str(autolib.bluecars_returned_sum.mode()))
print('The mode of slots freed: ' +str(autolib.slots_freed_sum.mode()))
print('The mode of slots taken: ' +str(autolib.slots_taken_sum.mode()))

# The modes of the numeric fields are unimodal, indicating the data is sampled from one population i.e. Paris

# range
print('The range of Bluecars taken: ' +str(autolib.bluecars_taken_sum.max() - autolib.bluecars_taken_sum.min()))
print('The range of Bluecars returned: ' +str(autolib.bluecars_returned_sum.max() - autolib.bluecars_returned_sum.min()))
print('The range of slots freed: ' +str(autolib.slots_freed_sum.max()- autolib.slots_freed_sum.min()))
print('The range of slots taken: ' +str(autolib.slots_taken_sum.max() - autolib.slots_taken_sum.min()))

# standard deviation
print('The standard deviation of Bluecars taken: ' +str(autolib.bluecars_taken_sum.std()))
print('The standard deviation of Bluecars returned: ' +str(autolib.bluecars_returned_sum.std()))
print('The standard deviation of slots freed: ' +str(autolib.slots_freed_sum.std()))
print('The standard deviation of slots taken: ' +str(autolib.slots_taken_sum.std()))

# The standard deviation of bluecars taken and those returned are almost similar.
# The standard deviation of slots taken and those freed are also almost similar.
# The standard deviations mirror the ranges, in that the values with more range show higher
# values of standard deviation, meaning the data is more spread out with increasing deviation.

# variance
print('The variance of Bluecars taken: ' +str(autolib.bluecars_taken_sum.var()))
print('The variance of Bluecars returned: ' +str(autolib.bluecars_returned_sum.var()))
print('The variance of slots freed: ' +str(autolib.slots_freed_sum.var()))
print('The variance of slots taken: ' +str(autolib.slots_taken_sum.var()))

# The variance of bluecars taken and those returned are almost similar.
# The variance of slots taken and those freed are also almost similar.

# Similar to standard deviation above in that, the greater the variance, the greater the spread 
# in the data about the mean.

# quantiles
print('The quantiles of Bluecars taken: \n' +str(autolib.bluecars_taken_sum.quantile([0.25,0.5,0.75])))
print('The quantiles of Bluecars returned: \n' +str(autolib.bluecars_returned_sum.quantile([0.25,0.5,0.75])))
print('The quantiles of slots freed: \n' +str(autolib.slots_freed_sum.quantile([0.25,0.5,0.75])))
print('The quantiles of slots taken: \n' +str(autolib.slots_taken_sum.quantile([0.25,0.5,0.75])))

# The quantiles of bluecars taken and those returned are similar.
# The quantiles of slots taken and those freed are also similar.

# skewness
print('The skewness of Bluecars taken: ' +str(autolib.bluecars_taken_sum.skew()))
print('The skewness of Bluecars returned: ' +str(autolib.bluecars_returned_sum.skew()))
print('The skewness of slots freed: ' +str(autolib.slots_freed_sum.skew()))
print('The skewness of slots taken: ' +str(autolib.slots_taken_sum.skew()))

# The positive values indicates that the tail of the data is right-skewed.

# kurtosis
print('The kurtosis of Bluecars taken: ' +str(autolib.bluecars_taken_sum.kurt()))
print('The kurtosis of Bluecars returned: ' +str(autolib.bluecars_returned_sum.kurt()))
print('The kurtosis of slots freed: ' +str(autolib.slots_freed_sum.kurt()))
print('The kurtosis of slots taken: ' +str(autolib.slots_taken_sum.kurt()))

# The data has positive kurtosis indicating that the distribution has heavier tails 
# and a taller peak than the normal distribution.

# Plotting Histogram to show the above

n_bins = 10

fig, (ax1,ax2,ax3, ax4) = plt.subplots(1, 4, figsize= (16,7))

sns.distplot(autolib.bluecars_taken_sum, ax=ax1, bins= n_bins)
sns.distplot(autolib.bluecars_returned_sum, ax=ax2, bins= n_bins)
sns.distplot(autolib.slots_freed_sum, ax=ax3, bins= n_bins)
sns.distplot(autolib.slots_taken_sum, ax=ax4, bins= n_bins)

# The data is right-skewed with a heavy tail as was discovered by the skewness and kurtosis.

"""**d) Univariate Analysis Recommendation**

The data is heavily skewed to the right i.e. leptokurtic, as was suspected due to the large number of outliers. This suggests that our initial decision to keep them is justified as this is not a normally distributed dataset. Furthermore, the Bluecar taken and returned columns seem to have similar statistical bearing, hence using either as the target variable in place of the other would be justified. We have decided to use the Bluecar taken column as our target variable, however, future analysis can be done with the Bluecar returned column and the results compared.

### 1.2 BIVARIATE

**a) Numeric**
"""

autolib.info()

Numerical_data = autolib.drop(['postal_code', 'date', 'n_daily_data_points', 'day_of_week', 
                               'day_type', 'utilib_taken_sum', 'utilib_returned_sum', 
                               'utilib_14_taken_sum', 'utilib_14_returned_sum'], axis=1)
sns.pairplot(Numerical_data)

# The plots below indicate a strong positive linear correlation between the variables.

pearson_coeff_Bluecar = autolib["bluecars_taken_sum"].corr(autolib["bluecars_returned_sum"], method="pearson") 
print('Pearson coefficient of Bluecar taken and returned is: ' + str(pearson_coeff_Bluecar))

pearson_coeff_Bluecar_Free = autolib["bluecars_taken_sum"].corr(autolib["slots_freed_sum"], method="pearson") 
print('Pearson coefficient of Bluecar taken and slots freed is: ' + str(pearson_coeff_Bluecar_Free))

pearson_coeff_Bluecar_Slots = autolib["bluecars_taken_sum"].corr(autolib["slots_taken_sum"], method="pearson") 
print('Pearson coefficient of Bluecar taken and slots taken is: ' + str(pearson_coeff_Bluecar_Slots))

pearson_coeff_Bluecar_return = autolib["bluecars_returned_sum"].corr(autolib["slots_freed_sum"], method="pearson") 
print('Pearson coefficient of Bluecar taken and slots freed is: ' + str(pearson_coeff_Bluecar_return))

pearson_coeff_Bluecar_return_slots = autolib["bluecars_returned_sum"].corr(autolib["slots_taken_sum"], method="pearson") 
print('Pearson coefficient of Bluecar taken and slots taken is: ' + str(pearson_coeff_Bluecar_return_slots))

pearson_coeff_slots = autolib["slots_taken_sum"].corr(autolib["slots_freed_sum"], method="pearson") 
print('Pearson coefficient of slots taken and slots freed is: ' + str(pearson_coeff_slots))

# Strong correlation is seen as was discovered on the sns plot, with all values above 0.94

sns.heatmap(Numerical_data.corr(), annot=True)
plt.show()

# This heatmap further cements the correlation found between the variables.

sns.pairplot(autolib)

# a larger view of the correlation in the data, just for reference.

"""**b) Categorical**"""

autolib.info()

fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4, figsize=(20, 7))
fig.suptitle('Categorical-Numerical Relationships')
sns.barplot(x= autolib.day_type, y= autolib.bluecars_taken_sum, ax=ax1)
sns.barplot(x= autolib.day_type, y= autolib.bluecars_returned_sum, ax=ax2)
sns.barplot(x= autolib.day_type, y= autolib.slots_freed_sum, ax=ax3)
sns.barplot(x= autolib.day_type, y= autolib.slots_taken_sum, ax=ax4)
plt.show()

# Contrary to what we believed in the univariate analysis, the weekend seems to have more activity.
# This could be due to the weekday having more days(5) than the weekend (2) days.

"""**c) Bivariate Analysis Recommendation**

From the above, we can see that the weekends are the busiest, and this could be due to weekend activities that people have time to engage in, requiring them to use the service. Even though this contradicts the univariate analysis, we will still use the weekday to conduct our hypothesis testing, since the weekdays have more days, hence more rows of data to work with. The more the data, the better our model will be.

# HYPOTHESIS TESTING

Determine if the mean of the number of blue_cars taken from postcodes starting with ‘75’ is at least similar to that of all the Paris postcodes. To investigate this, our hypothesis will be: 

a) The Null Hypothesis is that the mean of blue_cars taken in postcodes starting with ‘75’ during the weekdays is greater than or equal to that of all the Paris postcodes during the weekdays.

b) The Alternate Hypothesis is that the mean of blue_cars taken in postcodes starting with ‘75’ during the weekdays is less than that of all the Paris postcodes during the weekdays.
"""

# Considering the two postcodes are in the city area of Paris, the city will be extracted
# from the larger Paris Metropolitan area via the postcodes, since the postcodes of Paris City start with ‘75’. 

autolib.postal_code.unique()

# Target Population

Target = autolib.copy(deep = True)
Target.postal_code = Target.postal_code.astype(str)
Target.postal_code.dtype

Target = Target.loc[Target.postal_code.str.startswith('75')]
Target.postal_code = Target.postal_code.astype(int)
Target.postal_code.dtype

Target.postal_code.unique()

print('The Paris City data has: ' + str(Target.shape[0]) + ' rows')

# Selecting weekdays only

Target = Target[Target.day_type == 'weekday']

print('The Paris City data for the weekdays has: ' + str(Target.shape[0]) + ' rows')

# As was expected, the weekday data contains more data then the weekends.

# We can see that all postcodes have 111 or 112 rows of data each

Target.postal_code.value_counts()

Sample = Target.copy(deep= True)
Sample.drop(['utilib_taken_sum','utilib_returned_sum','utilib_14_taken_sum','utilib_14_returned_sum'], 
            axis = 1, inplace= True)
Sample = Sample.groupby('postal_code', group_keys=False).apply(lambda x: x.sample(30, random_state=10))
Sample

# previuosly we found population mean = 125.927
# and population standard deviation = 185.427

population_mean = autolib.bluecars_returned_sum.mean()
population_deviation = autolib.bluecars_taken_sum.std()
sample_mean = Sample.bluecars_taken_sum.mean()
sample_deviation = Sample.bluecars_taken_sum.std()

population_mean, population_deviation, sample_mean, sample_deviation

# Calculating the z-score

z = (sample_mean - population_mean) / population_deviation
print('The Z-score is: ', z)


# The z-score tells us that the sample mean is 1.23 standard deviations away from the population mean
# this is within the 1.645 critical value (since it is a one-tailed test), which is the area where 95% confidence level lies. We can therefore 
# not reject the null hypothesis.

p_value = 1 - stats.norm.cdf(z)
p_value

# The p value is greater than the alpha therefore, it is not statistically significant. 
# This indicates strong evidence for the null hypothesis.

# Additional Information

point_estimate = population_mean - sample_mean
point_estimate

# Additional Information

sample_size = Sample.shape[0]
std_error = sample_deviation / np.sqrt(sample_size)

stats.norm.interval(0.95, loc=sample_mean, scale=std_error)

# The interval estimate of the sample mean indicates the range through which the sample
# mean will be within the 95% Confidence level of the population mean.

"""**Test Sensitivity**"""

# Effect of changing the sample size, using a sample size of 90 and a random_state of 20

Sample = Target.copy(deep= True)
Sample.drop(['utilib_taken_sum','utilib_returned_sum','utilib_14_taken_sum','utilib_14_returned_sum'], 
            axis = 1, inplace= True)
Sample = Sample.groupby('postal_code', group_keys=False).apply(lambda x: x.sample(90, random_state=20))
sample_mean = Sample.bluecars_taken_sum.mean()
sample_deviation = Sample.bluecars_taken_sum.std()

population_mean, population_deviation, sample_mean, sample_deviation

z = (sample_mean - population_mean) / population_deviation
print('The Z-score is: ', z)

p_value = 1 - stats.norm.cdf(z)
p_value

# As can be seen, there is no significant difference, proving the rigidity of our method.

"""CONCLUSION

The Test sensitivity was done by changing the sample size to 90 samples per postcode. There was no significant difference, proving the rigidity of the method. Considering the mean for the ‘75’ code postcodes was higher than the mean of all the postcodes, the Z-score (1.23) being less than the Z-critical (1.625), and the p value (0.109) being greater than the alpha (0.05), the Null Hypothesis failed to be rejected. This means that indeed, these postcodes that start with ‘75’ i.e. those related to regions within the City Centre and it’s immediate environs, have a higher mean of bluecars being taken daily.

# RECOMMENDATION

The test suggests that the City Centre area of Paris experiences more customer engagement with the Bluecars. This suggests that the economic activities related to the City have an effect on the rate at which Bluecars are used. For future research, data regarding why the customer used a bluecar, e.g. Leisure, Work, Tourism etc, should be gathered to enable analysis that will show which activities boosted the use of the Bluecars most.
"""